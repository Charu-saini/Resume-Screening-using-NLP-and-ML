# -*- coding: utf-8 -*-
"""Resume Screening.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_p_jqFHQMqIOr6mBOoNUCc8FrRqtv31K
"""

import pandas as pd
import numpy as np

"""# Load the dataset"""

df_Category = pd.read_csv("/content/UpdatedResumeDataSet.csv")
df_Category

!sed -n 21230,21240p /content/jobs_dataset_with_features.csv

import pandas as pd

df_Role = pd.read_csv(
    "/content/jobs_dataset_with_features.csv",
    engine="python",
    on_bad_lines="skip"   # skip broken rows instead of crashing
)

print(df_Role.shape)
df_Role.head()

df_Role.columns

df_Category.columns

# Drop NaN rows if any
df_Role = df_Role.dropna()
df_Category = df_Category.dropna()

"""# Checking different categories"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,6))
sns.countplot(y=df_Category['Category'], order=df_Category['Category'].value_counts().index, palette="viridis")
plt.title("Distribution of Resume Categories", fontsize=14)
plt.xlabel("Count")
plt.ylabel("Category")
plt.show()

df_Role['Role'].nunique()

"""# Checking different roles"""

df_Role['Role'].unique()

df_Category['Category'].nunique()

df_Category['Category'].unique()

plt.figure(figsize=(10,6))
sns.countplot(y=df_Role['Role'], order=df_Role['Role'].value_counts().head(15).index, palette="plasma")
plt.title("Top 15 Roles in Dataset", fontsize=14)
plt.xlabel("Count")
plt.ylabel("Role")
plt.show()

"""# Checking length of resumes"""

df_Role['resume_length'] = df_Role['Features'].apply(lambda x: len(str(x).split()))
df_Category['resume_length'] = df_Category['Resume'].apply(lambda x: len(str(x).split()))

plt.figure(figsize=(10,5))
sns.histplot(df_Role['resume_length'], bins=50, color="purple", kde=True)
plt.title("Distribution of Resume Lengths (Roles Dataset)")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")
plt.show()

sns.histplot(df_Category['resume_length'], bins=50, color="purple", kde=True)
plt.title("Distribution of Resume Lengths (Roles Dataset)")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")
plt.show()

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

"""# NLP techniques"""

stop_words = set(stopwords.words("english"))

def preprocess(text):
    # Lowercase
    text = text.lower()
    # Remove non-alphabetic chars
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    # Remove stopwords
    text = " ".join([word for word in text.split() if word not in stop_words])
    return text

df_Role['cleaned_resume'] = df_Role['Features'].apply(preprocess)
df_Category['cleaned_resume'] = df_Category['Resume'].apply(preprocess)

"""# WordCloud"""

from wordcloud import WordCloud

text = " ".join(df_Role['Features'].astype(str).tolist())

wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)

plt.figure(figsize=(12,6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of Resume Texts")
plt.show()

df_Category.head()

df_Category.columns

text = " ".join(df_Category['Resume'].astype(str).tolist())

wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)

plt.figure(figsize=(12,6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of Resume Texts")
plt.show()

"""# TfidfVectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

# For category model
tfidf_category = TfidfVectorizer(max_features=5000)
X_category = tfidf_category.fit_transform(df_Category['cleaned_resume']).toarray()
y_category = df_Category['Category']

# For role model
tfidf_role = TfidfVectorizer(max_features=5000)
X_role = tfidf_role.fit_transform(df_Role['cleaned_resume']).toarray()
y_role = df_Role['Role']

from sklearn.model_selection import train_test_split

# Category split
Xc_train, Xc_test, yc_train, yc_test = train_test_split(X_category, y_category, test_size=0.2, random_state=42)

# Role split
Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_role, y_role, test_size=0.2, random_state=42)

"""# Training the model"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Category model
model_category = LogisticRegression(max_iter=1000)
model_category.fit(Xc_train, yc_train)
print("Category Classification Report:\n")
print(classification_report(yc_test, model_category.predict(Xc_test)))

# Role model
model_role = LogisticRegression(max_iter=1000)
model_role.fit(Xr_train, yr_train)
print("Role Classification Report:\n")
print(classification_report(yr_test, model_role.predict(Xr_test)))

"""# Commonly occuring words"""

import numpy as np

def show_top_features(model, vectorizer, n=15):
    for idx, label in enumerate(model.classes_):
        topn = np.argsort(model.coef_[idx])[-n:]
        print(f"Top words for {label}:")
        print([vectorizer.get_feature_names_out()[i] for i in topn][::-1])
        print("")

show_top_features(model_category, tfidf_category, n=10)

from sklearn.metrics import confusion_matrix

y_pred_cat = model_category.predict(Xc_test)

cm = confusion_matrix(yc_test, y_pred_cat, labels=model_category.classes_)

plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=False, cmap="Blues", xticklabels=model_category.classes_, yticklabels=model_category.classes_)
plt.title("Confusion Matrix - Category Model")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

from google.colab import drive
drive.mount('/content/drive')

import os

folder_path = '/content/drive/MyDrive/photos/'
os.makedirs(folder_path, exist_ok=True)  # creates folder if it doesn't exist

import pickle

pickle.dump(model_category, open(folder_path + "model_category.pkl", "wb"))
pickle.dump(tfidf_category, open(folder_path + "tfidf_category.pkl", "wb"))
pickle.dump(model_role, open(folder_path + "model_role.pkl", "wb"))
pickle.dump(tfidf_role, open(folder_path + "tfidf_role.pkl", "wb"))

os.listdir(folder_path)

